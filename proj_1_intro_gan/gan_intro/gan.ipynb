{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of distribution approximation using Generative Adversarial Networks in TensorFlow.\n",
    "\n",
    "Based on the blog post by Eric Jang: \n",
    "http://blog.evjang.com/2016/06/generative-adversarial-nets-in.html, and of course the original GAN paper by Ian Goodfellow et. al.: \n",
    "https://arxiv.org/abs/1406.2661\n",
    "\n",
    "The minibatch discrimination technique is taken from Tim Salimans et. al.: https://arxiv.org/abs/1606.03498."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "\n",
    "# Set metadata for library functions\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataDistribution(object):\n",
    "    def __init__(self):\n",
    "        self.mu = 4\n",
    "        self.sigma = 0.5\n",
    "\n",
    "    def sample(self, N):\n",
    "        samples = np.random.normal(self.mu, self.sigma, N)\n",
    "        samples.sort()\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeneratorDistribution(object):\n",
    "    def __init__(self, range):\n",
    "        self.range = range\n",
    "\n",
    "    def sample(self, N):\n",
    "        return np.linspace(-self.range, self.range, N) + \\\n",
    "            np.random.random(N) * 0.01\n",
    "\n",
    "\n",
    "def linear(input, output_dim, scope=None, stddev=1.0):\n",
    "    norm = tf.random_normal_initializer(stddev=stddev)\n",
    "    const = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope(scope or 'linear'):\n",
    "        w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm)\n",
    "        b = tf.get_variable('b', [output_dim], initializer=const)\n",
    "        return tf.matmul(input, w) + b\n",
    "\n",
    "\n",
    "def generator(input, h_dim):\n",
    "    h0 = tf.nn.softplus(linear(input, h_dim, 'g0'))\n",
    "    h1 = linear(h0, 1, 'g1')\n",
    "    return h1\n",
    "\n",
    "\n",
    "def discriminator(input, h_dim, minibatch_layer=True):\n",
    "    h0 = tf.tanh(linear(input, h_dim * 2, 'd0'))\n",
    "    h1 = tf.tanh(linear(h0, h_dim * 2, 'd1'))\n",
    "\n",
    "    # without the minibatch layer, the discriminator needs an additional layer\n",
    "    # to have enough capacity to separate the two distributions correctly\n",
    "    if minibatch_layer:\n",
    "        h2 = minibatch(h1)\n",
    "    else:\n",
    "        h2 = tf.tanh(linear(h1, h_dim * 2, scope='d2'))\n",
    "\n",
    "    h3 = tf.sigmoid(linear(h2, 1, scope='d3'))\n",
    "    return h3\n",
    "\n",
    "\n",
    "def minibatch(input, num_kernels=5, kernel_dim=3):\n",
    "    x = linear(input, num_kernels * kernel_dim, scope='minibatch', stddev=0.02)\n",
    "    activation = tf.reshape(x, (-1, num_kernels, kernel_dim))\n",
    "    diffs = tf.expand_dims(activation, 3) - tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)\n",
    "    abs_diffs = tf.reduce_sum(tf.abs(diffs), 2)\n",
    "    minibatch_features = tf.reduce_sum(tf.exp(-abs_diffs), 2)\n",
    "    return tf.concat(1, [input, minibatch_features])\n",
    "\n",
    "\n",
    "def optimizer(loss, var_list, initial_learning_rate):\n",
    "    decay = 0.95\n",
    "    num_decay_steps = 150\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        initial_learning_rate,\n",
    "        batch,\n",
    "        num_decay_steps,\n",
    "        decay,\n",
    "        staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        loss,\n",
    "        global_step=batch,\n",
    "        var_list=var_list\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "class GAN(object):\n",
    "    def __init__(self, data, gen, num_steps, batch_size, minibatch, log_every, anim_path=None):\n",
    "        self.data = data\n",
    "        self.gen = gen\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.minibatch = minibatch\n",
    "        self.log_every = log_every\n",
    "        self.mlp_hidden_size = 4\n",
    "        self.anim_path = anim_path\n",
    "        self.anim_frames = []\n",
    "\n",
    "        # can use a higher learning rate when not using the minibatch layer\n",
    "        if self.minibatch:\n",
    "            self.learning_rate = 0.005\n",
    "        else:\n",
    "            self.learning_rate = 0.03\n",
    "\n",
    "        self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        # In order to make sure that the discriminator is providing useful gradient\n",
    "        # information to the generator from the start, we're going to pretrain the\n",
    "        # discriminator using a maximum likelihood objective. We define the network\n",
    "        # for this pretraining step scoped as D_pre.\n",
    "        with tf.variable_scope('D_pre'):\n",
    "            self.pre_input = tf.placeholder(tf.float32, shape=(self.batch_size, 1))\n",
    "            self.pre_labels = tf.placeholder(tf.float32, shape=(self.batch_size, 1))\n",
    "            D_pre = discriminator(self.pre_input, self.mlp_hidden_size, self.minibatch)\n",
    "            self.pre_loss = tf.reduce_mean(tf.square(D_pre - self.pre_labels))\n",
    "            self.pre_opt = optimizer(self.pre_loss, None, self.learning_rate)\n",
    "\n",
    "        # This defines the generator network - it takes samples from a noise\n",
    "        # distribution as input, and passes them through an MLP.\n",
    "        with tf.variable_scope('G'):\n",
    "            self.z = tf.placeholder(tf.float32, shape=(self.batch_size, 1))\n",
    "            self.G = generator(self.z, self.mlp_hidden_size)\n",
    "\n",
    "        # The discriminator tries to tell the difference between samples from the\n",
    "        # true data distribution (self.x) and the generated samples (self.z).\n",
    "        #\n",
    "        # Here we create two copies of the discriminator network (that share parameters),\n",
    "        # as you cannot use the same network with different inputs in TensorFlow.\n",
    "        with tf.variable_scope('D') as scope:\n",
    "            self.x = tf.placeholder(tf.float32, shape=(self.batch_size, 1))\n",
    "            self.D1 = discriminator(self.x, self.mlp_hidden_size, self.minibatch)\n",
    "            scope.reuse_variables()\n",
    "            self.D2 = discriminator(self.G, self.mlp_hidden_size, self.minibatch)\n",
    "\n",
    "        # Define the loss for discriminator and generator networks (see the original\n",
    "        # paper for details), and create optimizers for both\n",
    "        self.loss_d = tf.reduce_mean(-tf.log(self.D1) - tf.log(1 - self.D2))\n",
    "        self.loss_g = tf.reduce_mean(-tf.log(self.D2))\n",
    "\n",
    "        vars = tf.trainable_variables()\n",
    "        self.d_pre_params = [v for v in vars if v.name.startswith('D_pre/')]\n",
    "        self.d_params = [v for v in vars if v.name.startswith('D/')]\n",
    "        self.g_params = [v for v in vars if v.name.startswith('G/')]\n",
    "\n",
    "        self.opt_d = optimizer(self.loss_d, self.d_params, self.learning_rate)\n",
    "        self.opt_g = optimizer(self.loss_g, self.g_params, self.learning_rate)\n",
    "\n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # pretraining discriminator\n",
    "            num_pretrain_steps = 1000\n",
    "            for step in xrange(num_pretrain_steps):\n",
    "                d = (np.random.random(self.batch_size) - 0.5) * 10.0\n",
    "                labels = norm.pdf(d, loc=self.data.mu, scale=self.data.sigma)\n",
    "                pretrain_loss, _ = session.run([self.pre_loss, self.pre_opt], {\n",
    "                    self.pre_input: np.reshape(d, (self.batch_size, 1)),\n",
    "                    self.pre_labels: np.reshape(labels, (self.batch_size, 1))\n",
    "                })\n",
    "            self.weightsD = session.run(self.d_pre_params)\n",
    "\n",
    "            # copy weights from pre-training over to new D network\n",
    "            for i, v in enumerate(self.d_params):\n",
    "                session.run(v.assign(self.weightsD[i]))\n",
    "\n",
    "            for step in xrange(self.num_steps):\n",
    "                # update discriminator\n",
    "                x = self.data.sample(self.batch_size)\n",
    "                z = self.gen.sample(self.batch_size)\n",
    "                loss_d, _ = session.run([self.loss_d, self.opt_d], {\n",
    "                    self.x: np.reshape(x, (self.batch_size, 1)),\n",
    "                    self.z: np.reshape(z, (self.batch_size, 1))\n",
    "                })\n",
    "\n",
    "                # update generator\n",
    "                z = self.gen.sample(self.batch_size)\n",
    "                loss_g, _ = session.run([self.loss_g, self.opt_g], {\n",
    "                    self.z: np.reshape(z, (self.batch_size, 1))\n",
    "                })\n",
    "\n",
    "                if step % self.log_every == 0:\n",
    "                    print('{}: {}\\t{}'.format(step, loss_d, loss_g))\n",
    "\n",
    "                if self.anim_path:\n",
    "                    self.anim_frames.append(self._samples(session))\n",
    "\n",
    "            if self.anim_path:\n",
    "                self._save_animation()\n",
    "            else:\n",
    "                self._plot_distributions(session)\n",
    "\n",
    "    def _samples(self, session, num_points=10000, num_bins=100):\n",
    "        '''\n",
    "        Return a tuple (db, pd, pg), where db is the current decision\n",
    "        boundary, pd is a histogram of samples from the data distribution,\n",
    "        and pg is a histogram of generated samples.\n",
    "        '''\n",
    "        xs = np.linspace(-self.gen.range, self.gen.range, num_points)\n",
    "        bins = np.linspace(-self.gen.range, self.gen.range, num_bins)\n",
    "\n",
    "        # decision boundary\n",
    "        db = np.zeros((num_points, 1))\n",
    "        for i in range(num_points // self.batch_size):\n",
    "            db[self.batch_size * i:self.batch_size * (i + 1)] = session.run(self.D1, {\n",
    "                self.x: np.reshape(\n",
    "                    xs[self.batch_size * i:self.batch_size * (i + 1)],\n",
    "                    (self.batch_size, 1)\n",
    "                )\n",
    "            })\n",
    "\n",
    "        # data distribution\n",
    "        d = self.data.sample(num_points)\n",
    "        pd, _ = np.histogram(d, bins=bins, density=True)\n",
    "\n",
    "        # generated samples\n",
    "        zs = np.linspace(-self.gen.range, self.gen.range, num_points)\n",
    "        g = np.zeros((num_points, 1))\n",
    "        for i in range(num_points // self.batch_size):\n",
    "            g[self.batch_size * i:self.batch_size * (i + 1)] = session.run(self.G, {\n",
    "                self.z: np.reshape(\n",
    "                    zs[self.batch_size * i:self.batch_size * (i + 1)],\n",
    "                    (self.batch_size, 1)\n",
    "                )\n",
    "            })\n",
    "        pg, _ = np.histogram(g, bins=bins, density=True)\n",
    "\n",
    "        return db, pd, pg\n",
    "\n",
    "    def _plot_distributions(self, session):\n",
    "        db, pd, pg = self._samples(session)\n",
    "        db_x = np.linspace(-self.gen.range, self.gen.range, len(db))\n",
    "        p_x = np.linspace(-self.gen.range, self.gen.range, len(pd))\n",
    "        f, ax = plt.subplots(1)\n",
    "        ax.plot(db_x, db, label='decision boundary')\n",
    "        ax.set_ylim(0, 1)\n",
    "        plt.plot(p_x, pd, label='real data')\n",
    "        plt.plot(p_x, pg, label='generated data')\n",
    "        plt.title('1D Generative Adversarial Network')\n",
    "        plt.xlabel('Data values')\n",
    "        plt.ylabel('Probability density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def _save_animation(self):\n",
    "        f, ax = plt.subplots(figsize=(6, 4))\n",
    "        f.suptitle('1D Generative Adversarial Network', fontsize=15)\n",
    "        plt.xlabel('Data values')\n",
    "        plt.ylabel('Probability density')\n",
    "        ax.set_xlim(-6, 6)\n",
    "        ax.set_ylim(0, 1.4)\n",
    "        line_db, = ax.plot([], [], label='decision boundary')\n",
    "        line_pd, = ax.plot([], [], label='real data')\n",
    "        line_pg, = ax.plot([], [], label='generated data')\n",
    "        frame_number = ax.text(\n",
    "            0.02,\n",
    "            0.95,\n",
    "            '',\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top',\n",
    "            transform=ax.transAxes\n",
    "        )\n",
    "        ax.legend()\n",
    "\n",
    "        db, pd, _ = self.anim_frames[0]\n",
    "        db_x = np.linspace(-self.gen.range, self.gen.range, len(db))\n",
    "        p_x = np.linspace(-self.gen.range, self.gen.range, len(pd))\n",
    "\n",
    "        def init():\n",
    "            line_db.set_data([], [])\n",
    "            line_pd.set_data([], [])\n",
    "            line_pg.set_data([], [])\n",
    "            frame_number.set_text('')\n",
    "            return (line_db, line_pd, line_pg, frame_number)\n",
    "\n",
    "        def animate(i):\n",
    "            frame_number.set_text(\n",
    "                'Frame: {}/{}'.format(i, len(self.anim_frames))\n",
    "            )\n",
    "            db, pd, pg = self.anim_frames[i]\n",
    "            line_db.set_data(db_x, db)\n",
    "            line_pd.set_data(p_x, pd)\n",
    "            line_pg.set_data(p_x, pg)\n",
    "            return (line_db, line_pd, line_pg, frame_number)\n",
    "\n",
    "        anim = animation.FuncAnimation(\n",
    "            f,\n",
    "            animate,\n",
    "            init_func=init,\n",
    "            frames=len(self.anim_frames),\n",
    "            blit=True\n",
    "        )\n",
    "        anim.save(self.anim_path, fps=30, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable D_pre/d0/w already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-111f964af16a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;31m# minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m       \u001b[1;31m# log_every\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[1;34m\"anim_output\"\u001b[0m      \u001b[1;31m# anim_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-11653b81d9a5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, gen, num_steps, batch_size, minibatch, log_every, anim_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.03\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-11653b81d9a5>\u001b[0m in \u001b[0;36m_create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mD_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_pre\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-11653b81d9a5>\u001b[0m in \u001b[0;36mdiscriminator\u001b[0;34m(input, h_dim, minibatch_layer)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'd0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'd1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-11653b81d9a5>\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, output_dim, scope, stddev)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m   1022\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    848\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32mC:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    344\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32mC:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    329\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    630\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 632\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    633\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable D_pre/d0/w already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "# Run the script\n",
    "mode = GAN(\n",
    "    DataDistribution(),\n",
    "    GeneratorDistribution(range=8),\n",
    "    1200,     # num_steps\n",
    "    12,       # batch_size\n",
    "    False,    # minibatch\n",
    "    10,       # log_every\n",
    "    'anim_output'      # anim_path\n",
    ")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for .py scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "#     model = GAN(\n",
    "#         DataDistribution(),\n",
    "#         GeneratorDistribution(range=8),\n",
    "#         args.num_steps,\n",
    "#         args.batch_size,\n",
    "#         args.minibatch,\n",
    "#         args.log_every,\n",
    "#         args.anim\n",
    "#     )\n",
    "#     model.train()\n",
    "\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--num-steps', type=int, default=1200,\n",
    "#                         help='the number of training steps to take')\n",
    "#     parser.add_argument('--batch-size', type=int, default=12,\n",
    "#                         help='the batch size')\n",
    "#     parser.add_argument('--minibatch', type=bool, default=False,\n",
    "#                         help='use minibatch discrimination')\n",
    "#     parser.add_argument('--log-every', type=int, default=10,\n",
    "#                         help='print loss after this many steps')\n",
    "#     parser.add_argument('--anim', type=str, default=None,\n",
    "#                         help='name of the output animation file (default: none)')\n",
    "#     return parser.parse_args()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main(parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
